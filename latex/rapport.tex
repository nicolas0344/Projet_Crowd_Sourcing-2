%%% packages %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\documentclass[frenchb]{report}
%\usepackage{natbib}
\usepackage[toc,page]{appendix}
\usepackage[dvipsnames]{xcolor}
\usepackage[french]{babel}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{vmargin}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{stmaryrd}
\usepackage{systeme}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{textcomp}
\usepackage{calrsfs}
\usepackage[T1]{fontenc}
\usepackage[toc,page]{appendix}
\usepackage{lipsum}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{adforn}
\usepackage{float}
\usepackage{subfig}
% Les packages necessaires pour faire le pseudo code
%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{algorithm}
\usepackage{algorithmic}
%%%%%%%%%%%%%%%%%%%%%%%%%
% Je renomme les commandes en français
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\algorithmicrequire}{\textbf{Entrée(s) :}}
\renewcommand{\algorithmicreturn}{\textbf{retourner}}
\renewcommand{\algorithmicensure}{\textbf{Initialisation ;}}
\renewcommand{\algorithmicwhile}{\textbf{Tant que}}
\renewcommand{\algorithmicdo}{\textbf{Initialisation}}
\renewcommand{\algorithmicendwhile}{\textbf{fin du Tant que ;}}
\renewcommand{\algorithmicend}{\textbf{fin}}
\renewcommand{\algorithmicif}{\textbf{si}}
\renewcommand{\algorithmicendif}{\textbf{fin du si}}
\renewcommand{\algorithmicelse}{\textbf{sinon}}
\renewcommand{\algorithmicelsif}{\textbf{fin du sinon}}
\renewcommand{\algorithmicthen}{\textbf{alors}}
\renewcommand{\algorithmicthen}{\textbf{Étape E}}
\renewcommand{\algorithmicthen}{\textbf{Étape M}}
\renewcommand{\algorithmicfor}{\textbf{pour}}
\renewcommand{\algorithmicforall}{\textbf{pour tout}}
\renewcommand{\algorithmicto}{\textbf{à}}
\renewcommand{\algorithmicendfor}{\textbf{fin du pour}}
\renewcommand{\algorithmicdo}{\textbf{faire}}
\renewcommand{\algorithmicloop}{\textbf{boucler}}
\renewcommand{\algorithmicendloop}{\textbf{fin de la boucle}}
\renewcommand{\algorithmicrepeat}{\textbf{répéter}}
\renewcommand{\algorithmicuntil}{\textbf{jusqu’à}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\hoffset}{-18pt}        
\setlength{\oddsidemargin}{0pt} % Marge gauche sur pages impaires
\setlength{\evensidemargin}{9pt} % Marge gauche sur pages paires
\setlength{\marginparwidth}{54pt} % Largeur de note dans la marge
\setlength{\textwidth}{481pt} % Largeur de la zone de texte (17cm)
\setlength{\voffset}{-18pt} % Bon pour DOS
\setlength{\marginparsep}{7pt} % Séparation de la marge
\setlength{\topmargin}{0pt} % Pas de marge en haut
\setlength{\headheight}{13pt} % Haut de page
\setlength{\headsep}{10pt} % Entre le haut de page et le texte
\setlength{\footskip}{27pt} % Bas de page + séparation
\setlength{\textheight}{708pt} % Hauteur de la zone de texte (25cm)
\usepackage{hyperref}
\lstset{%
            inputencoding=utf8,
                extendedchars=true,
                literate=%
                {é}{{\'e}}{1}%
                {è}{{\`e}}{1}%
                {à}{{\`a}}{1}%
                {ç}{{\c{c}}}{1}%
                {œ}{{\oe}}{1}%
                {ù}{{\`u}}{1}%
                {É}{{\'E}}{1}%
                {È}{{\`E}}{1}%
                {À}{{\`A}}{1}%
                {Ç}{{\c{C}}}{1}%
                {Œ}{{\OE}}{1}%
                {Ê}{{\^E}}{1}%
                {ê}{{\^e}}{1}%
                {î}{{\^i}}{1}%
                {ô}{{\^o}}{1}%
                {û}{{\^u}}{1}%
                {ë}{{\¨{e}}}1
                {û}{{\^{u}}}1
                {â}{{\^{a}}}1
                {Â}{{\^{A}}}1
                {Î}{{\^{I}}}1
        }
    
    
\lstset{language=R,
  backgroundcolor=\color{lightgray},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument %MidnightBlue
   basicstyle=\small\ttfamily\color{white},        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{SpringGreen},    % comment style
  deletekeywords={data,frame,length,as,character},           % if you want to delete keywords from the given language
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
   keywordstyle=\color{Peach},       % keyword style
  morekeywords={kable,*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  %numberstyle=\tiny\color{gray}, % the style that is used for the line-numbers
  rulecolor=\color{white},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
      % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  mathescape=true,
  escapechar=|
  }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%        


        
\makeatletter
\let\thetitle\@title
\let\theauthor\@author
\let\thedate\@date
\makeatother

%%% commandes mise en page %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%        
\newcommand{\ld}{\log_{2}}
\newcommand{\R}{\mathbbm{R}}
\newcommand{\N}{\mathbbm{N}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\E}{\mathbbm{E}}
\newcommand{\V}{\mathbbm{V}}
\newcommand{\prob}{\mathbbm{P}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\Xti}{\widetilde{X_i}}
\newcommand{\Xtj}{\widetilde{X_j}}
\newcommand{\Xn}{\overline{X_n}}
\newcommand{\gn}{\hat{g_n}}
\newcommand{\n}{\mathcal{N}}
\newcommand{\lv}{\mathcal{L}}
\newcommand{\thetat}{\tilde{\theta}}
\newcommand{\indep}{\perp \!\!\! \perp}

\newcommand{\console}[1]{\colorbox{black}{\begin{minipage}[c]{1\linewidth}\textcolor{white}{\texttt{#1}}\end{minipage}}}

\newtheorem{prop}{Proposition}
\newtheorem{thm}{Théorème}
\newtheorem{cor}{Corollaire}
\newtheorem{lem}{Lemme}
\newtheorem{hyp}{Hypothèse}
\theoremstyle{definition}\newtheorem{defn}{Définition}
\theoremstyle{definition}\newtheorem{exm}{Exemple}
\theoremstyle{definition}\newtheorem{nota}{Notation}
\theoremstyle{definition}\newtheorem{rem}{Remarque}

\renewcommand{\qedsymbol}{\adfhangingflatleafright}


\begin{document}
%%% Pour l'annexe
\def\appendixpage{\vspace*{8cm}
\begin{center}
\Huge\textbf{Annexes}
\end{center}
}
\def\appendixname{Annexe}%
%%%

%%% Page de garde %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
\begin{center}
\includegraphics[scale=0.6]{logo.png}
\hfill
\includegraphics[scale=0.35]{fds_logo.png}\\[3cm]
\linespread{1.2}\huge {\bfseries Projet Master 2 SSD }\\[0.5cm]
\linespread{1.2}\LARGE {\bfseries Crowd-Sourcing}\\[1.5cm]
\linespread{1}

{\large Rédigé par\\}
{\Large \textsc{pralon} Nicolas}\\
{\Large \textsc{thiriet} Aurelien}\\
{\large \emph{Encadrant :} Joseph \textsc{Salmon}}\\[1.5cm] 

\includegraphics[scale=0.7]{imag_logo.png}

\end{center}
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}
\addcontentsline{toc}{part}{Introduction}

En science appliquée et notamment en statistique inférentielle, le recueil de données constitue une étape primordiale, il nous permet d'élaborer des modèles et de construire des estimateurs. Il convient à ce titre d'être attentif à l'observation des données.

Dans un cadre idéal, les modèles construits nécessitent un faible nombre de données dont l'observation peut être réalisée par des professionnels du domaine concerné. Toujours est-il que ces situations sont peu courantes et l'observation, par ces experts, d'un grand nombre de données n'est pas envisageable. C'est à ce titre que le "Crowd-Sourcing" est couramment utilisé, puisqu'un grand nombre d'intervenants permet de construire la base de donnée nécessaire. Toutefois des erreurs d'obervations quant à la nature des données peut-être commises, il est alors essentiel de s'accorder sur une décision.

C'est à ce problème, que ce projet propose de présenter différentes solutions. Nous étudierons le modèle construit par DAWID et SKENE.


\newpage

\chapter{Modélisation du problème}

\section{Modèle Dawid-Skene}

Dans ce chapitre, nous établissons le cadre nous permettant de traiter le problème de décision lorsque plusieurs obervations d'une même donnée sont fournies. 

Nous définissons alors l'exemple d'un ensemble d'images à labeliser, les experts du domaines décident alors de faire appel à plusieurs participants pour labéliser ces images.\\
On définit le cadre suivant : 

\begin{itemize}[label=\adfflowerleft]
	\item On dispose d'un esemble d'images $\left\{1,\cdots,I\right\}$ et on suppose le vecteur aléatoire $(X_i)_{i \in \{1,\cdots,I \}}$, tel que $\forall i$, $X_i$ à valeur dans $\left\{1,\cdots,J\right\}$ correspond à l'objet illustré sur l'image $i$.
	\item On dispose egalement d'un ensemble de participants (annotateurs) $\{1, \cdots, K\}$, et on considère $\forall i \in \{1, \cdots, I\}$ le vecteur aléatoire $(Y^k_i)_{k \in \{1, \cdots, K\}}$ la labélisation du participant $k$ à l'image $i$, $Y^k_i$ à valeur dans $\{0,1\}^J$ (par exemple \{0,...,0,1,0,..,0\} si le participant annote une seule fois l'image $k$ comme ce sera le cas dans la partie 2). 
	\item Chaque participant répond indépendamment des autres et chaque image est indépendante des autres, elle ne donne aucune indication sur les autres images . On note l'indépendance 2 à 2, $(Y^k_i)_k \indep$ et $(X_i)_i \indep$
\end{itemize}

A partir des résultats donnés par chaque participant nous pouvons essayer d'attribuer un label à chaque image. Toutefois nous pouvons nous retrouver dans la situation où le label donné par un participant à une image diffère de celui d'un autre participant, pour diverses raisons; erreur d'annotation, fatigue, spam. Il convient dans ce cas de trouver une solution à la question : quel label donner à chaque image ? \\

Afin de simplifier les démarches dans un premier temps, nous allons envisager le cadre suivant; nous allons considérer, en plus de l'observation des résultats des participants, observer les labels des images. \\
Ce cadre s'écarte légèrement de celui du problème puisque l'information sur les labels des images n'est pas accessible, cependant nous verrons comment pallier cela dans la partie suivante; notamment par une garantie théorique d'un algorithme que l'on appelle algorithme EM. \\
Dans un premier temps nous considérons le cas d'un participant et une image.\\

Nous émettons l'hypothèse suivante : 

\begin{hyp}
	Nous supposons que, $\forall j,k \in \llbracket 1,J \rrbracket \times \llbracket 1,K \rrbracket$, $Y^k_i | X_i = \alpha_i \sim Multinomiale(n,(\pi^k_{\alpha_i,j})_{j \in \llbracket 1,J \rrbracket})$
\end{hyp}

Sachant que l'image $i$ a pour label $\alpha_i$, la réponse du participant $k$ suit une loi multinomiale. Dans le cas où le label de l'image $i$ est connu, le participant $k$ peut aussi y avoir accès, et nous avons $Y^k_i, X_i $ non indépendant. \\

Chaque paramètre $(\pi^k_{\alpha_i,j})_{j \in \llbracket 1,J \rrbracket}$ correspond à la probabilité que le participant $k$  attribue le label $j$ à l'image $i$ sachant que le label correct est le label $\alpha_i$.\\

\newpage
On a ainsi :

\begin{center}
$\prob_{Y^k_i|X_i = \alpha_i} = \displaystyle \sum_{(n^k_{i,1}, \cdots, n^k_{i,J})} \underbrace{\frac{\big(\displaystyle \sum_{j=1}^J n^k_{i,j}\big)!}{\displaystyle \prod_{j=1}^J (n^k_{i,j})!}\displaystyle \prod_{j=1}^J \big(\pi^k_{\alpha_i,j}\big)^{n^k_{i,j}}}_{\textit{densité suivant} \displaystyle \sum \delta_{(n^k_{i,1}, \cdots, n^k_{i,J})}} \delta_{(n^k_{i,1}, \cdots, n^k_{i,J})}$
\end{center}

Avec $n^k_{i,j}$ le nombre de fois que le participant $k$ a attribué le label $j$ à l'image $i$\\

On a alors que la vraisemblance de $Y^k_i | X_i = \alpha_i$ est proportionnelle à $\displaystyle \prod_{j=1}^J \big(\pi^k_{\alpha_i,j}\big)^{n^k_{i,j}}$\\


\begin{rem}
	Ici on a une dépendance des paramètres $(\pi^k_{\alpha_i,j})$ en $k$,$j$ et $\alpha_i$ (pour le conditionnement), mais l'indice $i$ sert simplement à indiquer qu'on considère l'image $i$ et ne consitue pas un paramètre.\\
	Cela signifie que, sachant que le label de l'image $i$ est $\alpha_i$ et que le label de l'image $i'$ est $\alpha_{i'}$ mais $\alpha_i = \alpha_{i'}$, la probabilité que le participant $k$ attribue le label $j$ à l'image $i$ est la même de celle où il attribue le label $j$ pour l'image $i'$. 
	En tout état de connaissance, le participant sachant que les deux images ont le même label, la probabilité qu'il annote le label $j$ à l'une, et $j$ à l'autre n'a pas lieu d'être différente. Il s'agit pour le particpant d'une même situation et le choix ne dépend que de lui. 
\end{rem}
\bigskip


Dans le but de se ramener au cas de plusieurs images et plusieurs participants, on considère ici une image $i$ et tous les participants $(1, \cdots, K)$.\\

\begin{rem}
	Par souci de lisibilité on notera $\prob(Y^k_i = (n^k_{i,1}, \cdots, n^k_{i,J})|X_i = \alpha_i)$ par $\prob(Y^k_i|X_i = \alpha_i).$
\end{rem}

Pour tous les participants on a dans ce cas : 
\begin{center}
	\begin{align*}
		\prob \left( \displaystyle \bigcap_{k=1}^K Y^k_i |X_i =\alpha_i \right) &= \displaystyle \prod_{k=1}^K \prob \left(Y^k_i |X_i = \alpha_i \right) \textit{, car} (Y^k_i)_k \indep\\
	&\propto \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{\alpha_i,j}\right)^{n^k_{i,j}}\\
	\end{align*}
\end{center}

La vraisemblance du vecteur $(Y^k_i | X_i = \alpha_i)_k$ est proportionnelle à cette valeur.\\

Ramenons nous à présent au cadre de notre problème et intéressons nous au vecteur $((Y^k_i)_k, X_i)$.\\
Nous pouvons de ce qui précède déterminer la probabilité $\prob \left( \left( \displaystyle \bigcap_k Y^k_i \right) \bigcap \left(X_i = \alpha_i \right) \right)$.\\

On note $\forall (i,j) \in \{1, \cdots, J\}^2, p_j = \prob(X_i = j)$.

\begin{center}
	\begin{align*}
		\prob \left( \left( \displaystyle \bigcap_k Y^k_i \right) \bigcap \left(X_i = \alpha_i \right) \right) &= \prob \left( \displaystyle \bigcap_{k = 1}^K Y^k_i |X_i = \alpha_i \right) \times \prob(X_i = \alpha_i)\\
		& \propto p_{\alpha_i} \times \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{\alpha_i,j}\right)^{n^k_{i,j}}\\
	\end{align*}
\end{center}

$p_{\alpha_i}$ désigne la probabilité qu'une image tirée au hasard ait le label $\alpha_i$.

Plus généralement, en définissant $T_{i,l}$ la variable aléatoire telle que $T_{i,l} = 1$, si le label de l'image  $i$ est $l$ et $T_{i,l} = 0$ sinon, on a : \\

\begin{center}
	$p_{\alpha_i} \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{\alpha_i,j}\right)^{n^k_{i,j}} = \displaystyle \prod_{l=1}^J \left( p_l \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{l,j}\right)^{n^k_{i,j}} \right)^{T_{i,l}}$
\end{center}

De plus l'événement $\{X_i = \alpha_i \} = \{T_{i,\alpha_i} = 1\}$, $\forall l \in \{1, \cdots, J\}$ $T_{i,l}$ est alors connu.\\

Il manque maintenant à se placer dans le cas où on observe les résutats des participants pour toutes les images et les labels de chaque image.\\

$\forall k, (Y^k_i) \indep X_{i'}, i' \neq i$ et $(Y^k_i)_i \indep $, on a alors $Y^k_i \indep (Y^k_{i'},X_{i'})$ puis $(Y^k_i,X_i) \indep (Y^k_{i'},X_{i'})$\\
On a ainsi la probabilité : 

\begin{center}
	\begin{align*}
		\prob \left( \left( \displaystyle \bigcap_i \displaystyle \bigcap_k Y^k_i \right) \cap \left( \displaystyle \bigcap_i Xi = \alpha_i \right) \right) &= \prob \left( \displaystyle \bigcap_i \left[ \left( \displaystyle \bigcap_k Y^k_i \right) \cap \left(X_i = \alpha_i\right) \right] \right)\\
	&= \displaystyle \prod_i \prob \left( \left( \displaystyle \bigcap_k Y^k_i \right) \cap \left( X_i = \alpha_i \right) \right)\\
	&\propto \displaystyle \prod_{i=1}^I \displaystyle \prod_{l=1}^J \left( p_l \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{l,j}\right)^{n^k_{i,j}} \right)^{T_{i,l}}\\
	\end{align*}
\end{center}

La vraisemblance est également proportionnelle à cette même expression.


\section{Résolution du problème}

Pour faire un choix quant au label de chaque image, la démarche que l'on adopte est la suivante :\\

Soit l'image $i$, on décide que le label de cette image est $l$ si : \\
\begin{center}
	$l = argmax_l (T_{i,l})$
\end{center}

Les paramètres $T_{i,l}$ n'étant pas connu on souhaite les estimer.\\
Un estimateur naturel pouvant être pris pour chaque $T_{i,l}$ est l'espérance conditionnelle sachant $(Y^k_i)_k$, $\E[T_{i,l}|(Y^k_i)_k]$. \\
Il s'agit de la meilleure approximation de $T_{i,l}$ au sens de la norme $\mathcal{L}^2$, à partir de ce que l'on connait $<(Y^k_i)_k>$ (les réponses des participants).\\

$\E[T_{i,l}|(Y^k_i)_k]$ peut se réécrire sous la forme suivante : 

\begin{center}
	\begin{align*}
		\E[T_{i,l}|(Y^k_i)_k] &= \E[0 \times \1_{\{T_{i,l} = 0\}} + 1 \times \1_{\{T_{i,l} = 1\}}|(Y^k_i)_k] \\
		&= \E[\1_{\{T_{i,l} = 1\}}|(Y^k_i)_k]\\
		&= \prob\left( T_{i,l} = 1 |(Y^k_i)_k \right)\\
	\end{align*}
\end{center}

En explicitant cette probabilité on a : 

\begin{center}
	\begin{align*}
		\prob\left( T_{i,l} = 1 |(Y^k_i)_k \right) &= \displaystyle \frac{\prob \left( \left( T_{i,l} = 1 \right) \cap \left(Y^k_i\right)_k\right)}{\prob \left( \left( Y^k_i \right)_k \right)}\\
		&= \displaystyle \frac{\prob \left( \left(Y^k_i \right)_k | \left( T_{i,l} =1 \right) \right) \times \prob (T_{i,l} = 1)}{\displaystyle \sum_l^J \prob \left( \left(Y^k_i \right)_k | \left( X_i = l \right) \right) \times \prob (X_i = l)}\\
		&= \displaystyle \frac{\prob \left( \left(Y^k_i \right)_k | \left( X_i = l \right) \right) \times \prob (X_i = l)}{\displaystyle \sum_l^J \prob \left( \left(Y^k_i \right)_k | \left( X_i = l \right) \right) \times \prob (X_i = l)}\\
		&= \displaystyle \frac{p_l \displaystyle \prod_k^K \displaystyle \prod_j^J (\pi^k_{l,j})^{n^k_{i,j}}}{\displaystyle \sum_l^J p_l \displaystyle \prod_k^K \displaystyle \prod_j^J (\pi^k_{l,j})^{n^k_{i,j}}}, \textit{car } \forall i,l ~\{X_i = l\} = \{T_{i,l} = 1\}\\
	\end{align*}
\end{center}


\begin{rem}
On a donc réussi à obtenir une estimation des $T_{i,l}$ dépendant des paramètres $(\pi^k_{l,j})_{k,l,j}$ et $p_l$. Cependant ces paramètres n'étant pas connus, il nous faut les estimer et on les estime par maximum de vraisemblance, à partir du modèle défini dans la partie précédente.\\
Un problème est présent, ceci étant dû au modèle que l'on a défini car, la détermination des estimateurs du maximum de vraisemblance nécessite de bénéficier des observations des labels des images, qui ne sont pas connus, en particulier il nous faut avoir accès aux valeurs des $T_{i,l}$ dans l'expression de la vraisemblance.\\

Ceci ne nous permet pas, dans ce cas, de donner les estimateurs du maximum de vraisemblance des $(\pi^k_{l,j})_{k,l,j}$ et $p_l$. \\
Toutefois, on voit apparaître une procédure permettant d'estimer les estimateurs du maximum de vraisemblance et l'espérance conditionnelle des $T_{i,l}$, il s'agit dans un premier temps, d'estimer arbitrairement les $T_{i,l}$ afin d'obtenir une estimation des estimateurs du maximum de vraisemblance $(\pi^k_{l,j})_{k,l,j}$ et $p_l$.\\
On peut alors donner une estimation des $\E[T_{i,l}|(Y^k_i)_k]$, et de nouveau obtenir une estimation des estimateurs du maximum de vraisemblance des $(\pi^k_{l,j})_{k,l,j}$ et $p_l$. Cette procédure itérative correspond à ce que l'on appelle l'algoritme EM.\\

Il n'existe pas de preuve de convergence de la suite de paramètres établie par l'algorithme EM vers les estimateurs définis, et le choix de bons paramètres initiaux est de fait primordial.\\
Comme sus-mentionné en début de partie, nous disposons d'une garantie théorique de l'algoritme EM justifiant la démarche que nous avons introduite;  celle-ci étant que les estimateurs ainsi obtenus ont la particularité de faire croitre, à chaque itération, la vraisemblance des observations. \\
Toutefois, ce résultat ne prétend pas que l'algorithme EM maximisera la vraisemblance des observations. Ce dernier peut tout à fait produire une suite de paramètres correspondant à un maximum local ; et une augmentation du nombre d'itérations n'apportera pas de solution à ce problème.\\
\end{rem}


\newpage

Considérerons connu les valeurs de $T{i,l}$, et déterminons à présent les expressions des estimateurs du maximum de vraisemblance.
\bigskip

En effectuant la maximisation du Lagrangien de la fonction de vraisemblance: 

\begin{center}
	$\left( \left(\pi^k_{l,j}\right)_{l,j,k},(p_j)_j \right) |  \!\!\! \longrightarrow \displaystyle \prod_i^I \displaystyle \prod_{l=1}^J \left( p_l \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{l,j}\right)^{n^k_{i,j}} \right)^{T_{i,l}}$
\end{center}

Sous la contrainte $\forall l,k \in  \llbracket 1,J \rrbracket \times  \llbracket 1,K \rrbracket, \left(\displaystyle \sum_{j = 1}^J \pi^k_{l,j} = 1 \right)$\\

On obtient les estimateurs du maximum de vraisemblance suivants : \\

\begin{center} 
	$\widehat{\pi^{k}_{l,j}} = \displaystyle \frac{\displaystyle \sum_i T_{i,l} n^k_{i,j}}{\displaystyle \sum_l \displaystyle \sum_i T_{i,l} n^k_{i,j}}$
\end{center}
\begin{center}
	$\widehat{p_l} = \displaystyle \frac{\displaystyle \sum_i T_{i,l}}{I}$
\end{center}

\begin{rem}
	La preuve alourdissant la lecture, on se contente de donner l'idée générale permettant de retrouver les estimateurs. \\
Il convient de faire ressortir de la fonction ci-dessus, pour l'estimation des $\pi^k_{l,j}$ (respectivement des $p_l$), le terme en $\pi^k_{l,j}$ (repectivement en $p_l$). On obtient alors les termes suivant $\displaystyle \prod_i^I \left(\pi^k_{l,j}\right)^{n^k_{i,j}T_{i,l}} a_i$ et $\displaystyle \prod_i^I (p_l)^{T_{i,l}} b_i$ avec $a_i$ et $b_i$ correspondant au reste du produit.\\
Les contraintes de maximisation permettent ensuite d'obtenir les dénominateurs dans l'expression des estimateurs. 
\end{rem}
\bigskip
\bigskip
\bigskip





\section{Initialisation des $T_{i,l}$}

On peut proposer une initialisation simple des  $T_{i,l}$ sans la base d'une quelconque information des données, en les estimant par : 

\begin{center}
	$T_{i,l} = \displaystyle \frac{1}{J}$
\end{center}

Mais cette initialisation n'est pas optimale pour espérer estimer converger vers les estimateurs du maximum de vraisemblance.\\

Si les participants annotent suffisament bien les images, on peut donner une meilleure initialisation des $T_{i,l}$ en se basant sur les résultats obtenus des participants, dans ce cas on estime $T_{i,l}$ par : 

\begin{center}
	 $T_{i,l} = \displaystyle \frac{ \displaystyle \sum_k n_{i,l}^k}{\displaystyle \sum_k \displaystyle \sum_j n_{i,j}^k}$
\end{center}

Il s'agit d'estimer $T_{i,l}$ par le nombre de fois où les participants ont annoté $l$ l'image $i$ sur l'ensemble des annotations qu'ils ont données pour l'image $i$, ici les $n_{i,l}^k$ valent soit $1$ soit $0$. On estime $T_{i,l}$ par une sorte de "vote majoritaire".

\chapter{Pseudo-Code algoritme EM}

Dans cette partie nous allons présenter l'algorithme EM, il s'agit d'une méthode itérative, constituée de deux étapes, à savoir une étape "Expectation" et une étape "Maximization". Cette algorithme nous permet de répondre à notre problèmatique initiale, par la garantie théorique de la croissance de la vraisemblance du modèle défini dans la partie Dawid-Skene.

\section{L'étape E (Expectation)}

La phase E consiste à calculer l'estimation des $T_{i,l}$ par $\E[T_{i,l}|(Y^k_i)_k]$.\\
Puisque seule son expression est nécessaire pour obtenir l'estimation des estimateurs du maximum de vraisemblance des $(\pi^k_{l,j})_{k,l,j}$ et $p_l$. Son calcul est rendu possible par la forme analytique que l'on a pû déterminer précédement à la page $6$, il convient dans un premier temps d'estimer les $(\pi^k_{l,j})_{k,l,j}$ et $p_l$ en ayant utilisé l'initialisation des $T_{i,l}$.

\section{L'étape M (Maximization)}

La phase M consiste à maximiser la vraisemblance du modèle, ce maximum est atteint en les paramètres $\left(\widehat{\pi^k_{l,j}}, \widehat{p_l} \right)$ et ces deux paramètres seront calculés à l'aide des maximas établis à la page $6$. Ce sont ces estimateurs qui seront utilisés dans l'itération suivante pour mettre à jour estimation à l'étape E.

\section{Pseudo-code}

\begin{algorithm}
	\caption{\textbf{L’algorithme EM (Dempster et al., 1977).}}
	\begin{algorithmic}[1]
		\REQUIRE{initialisation des $(T_{i,l})_{i,l} = (T_{i,l})_0$, les observations $\left(Y^k_{i}\right)_{k,i}$, $(n_{i,l}^k)_{k,i,l}$ et $N \in \mathbb{N}$ le nombre d'intération;}
		\FOR {$n $ allant de $1$ à $N$}
		\STATE {$\text{\textbf{ETAPE M :} \textit{Calculer les }} \left(\widehat{\pi^k_{l,j}}\right)_n = \displaystyle \frac{\displaystyle \sum_i \left(T_{i,l}\right)_{n-1} n^k_{i,j}}{\displaystyle \sum_l \displaystyle \sum_i \left(T_{i,l}\right)_{n-1} n^k_{i,j}} $, $\left(\widehat{p_l}\right)_n = \displaystyle \frac{\displaystyle \sum_i \left(T_{i,l}\right)_{n-1}}{I}$}
		\STATE {$\text{\textbf{ETAPE E :}\textit{ Calculer }} \left(\widehat{T_{i,l}}\right)_n = \displaystyle \frac{ \left(\widehat{p_l}\right)_n \displaystyle \prod_k^K \displaystyle \prod_j^J \left(\widehat{\pi^k_{l,j}}\right)_n^{n^k_{i,j}}}{\displaystyle \sum_l^J \left(\widehat{p_l}\right)_n \displaystyle \prod_k^K \displaystyle \prod_j^J \left(\widehat{\pi^k_{l,j}}\right)_n^{n^k_{i,j}}}$;}
		\ENDFOR
		\RETURN {$max_{_l} \left(\widehat{T_{i,l}}\right)_N $;}
	\end{algorithmic}
\end{algorithm}


\end{document}
