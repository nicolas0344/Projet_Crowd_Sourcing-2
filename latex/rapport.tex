%%% packages %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\documentclass[frenchb]{report}
%\usepackage{natbib}
\usepackage[toc,page]{appendix}
\usepackage[dvipsnames]{xcolor}
\usepackage[french]{babel}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{vmargin}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{stmaryrd}
\usepackage{systeme}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{textcomp}
\usepackage{calrsfs}
\usepackage[T1]{fontenc}
\usepackage[toc,page]{appendix}
\usepackage{lipsum}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{adforn}
\usepackage{float}
\usepackage{subfig}
% Les packages necessaires pour faire le pseudo code
%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{algorithm}
\usepackage{algorithmic}
%%%%%%%%%%%%%%%%%%%%%%%%%
% Je renomme les commandes en français
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\algorithmicrequire}{\textbf{Entrée(s) :}}
\renewcommand{\algorithmicreturn}{\textbf{retourner}}
\renewcommand{\algorithmicensure}{\textbf{Initialisation ;}}
\renewcommand{\algorithmicwhile}{\textbf{Tant que}}
\renewcommand{\algorithmicdo}{\textbf{Initialisation}}
\renewcommand{\algorithmicendwhile}{\textbf{fin du Tant que ;}}
\renewcommand{\algorithmicend}{\textbf{fin}}
\renewcommand{\algorithmicif}{\textbf{si}}
\renewcommand{\algorithmicendif}{\textbf{fin du si}}
\renewcommand{\algorithmicelse}{\textbf{sinon}}
\renewcommand{\algorithmicelsif}{\textbf{fin du sinon}}
\renewcommand{\algorithmicthen}{\textbf{alors}}
\renewcommand{\algorithmicthen}{\textbf{Étape E}}
\renewcommand{\algorithmicthen}{\textbf{Étape M}}
\renewcommand{\algorithmicfor}{\textbf{pour}}
\renewcommand{\algorithmicforall}{\textbf{pour tout}}
\renewcommand{\algorithmicto}{\textbf{à}}
\renewcommand{\algorithmicendfor}{\textbf{fin du pour}}
\renewcommand{\algorithmicdo}{\textbf{faire}}
\renewcommand{\algorithmicloop}{\textbf{boucler}}
\renewcommand{\algorithmicendloop}{\textbf{fin de la boucle}}
\renewcommand{\algorithmicrepeat}{\textbf{répéter}}
\renewcommand{\algorithmicuntil}{\textbf{jusqu’à}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\hoffset}{-18pt}        
\setlength{\oddsidemargin}{0pt} % Marge gauche sur pages impaires
\setlength{\evensidemargin}{9pt} % Marge gauche sur pages paires
\setlength{\marginparwidth}{54pt} % Largeur de note dans la marge
\setlength{\textwidth}{481pt} % Largeur de la zone de texte (17cm)
\setlength{\voffset}{-18pt} % Bon pour DOS
\setlength{\marginparsep}{7pt} % Séparation de la marge
\setlength{\topmargin}{0pt} % Pas de marge en haut
\setlength{\headheight}{13pt} % Haut de page
\setlength{\headsep}{10pt} % Entre le haut de page et le texte
\setlength{\footskip}{27pt} % Bas de page + séparation
\setlength{\textheight}{708pt} % Hauteur de la zone de texte (25cm)
\usepackage{hyperref}
\lstset{%
            inputencoding=utf8,
                extendedchars=true,
                literate=%
                {é}{{\'e}}{1}%
                {è}{{\`e}}{1}%
                {à}{{\`a}}{1}%
                {ç}{{\c{c}}}{1}%
                {œ}{{\oe}}{1}%
                {ù}{{\`u}}{1}%
                {É}{{\'E}}{1}%
                {È}{{\`E}}{1}%
                {À}{{\`A}}{1}%
                {Ç}{{\c{C}}}{1}%
                {Œ}{{\OE}}{1}%
                {Ê}{{\^E}}{1}%
                {ê}{{\^e}}{1}%
                {î}{{\^i}}{1}%
                {ô}{{\^o}}{1}%
                {û}{{\^u}}{1}%
                {ë}{{\¨{e}}}1
                {û}{{\^{u}}}1
                {â}{{\^{a}}}1
                {Â}{{\^{A}}}1
                {Î}{{\^{I}}}1
        }
    
    
\lstset{language=R,
  backgroundcolor=\color{lightgray},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument %MidnightBlue
   basicstyle=\small\ttfamily\color{white},        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{SpringGreen},    % comment style
  deletekeywords={data,frame,length,as,character},           % if you want to delete keywords from the given language
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
   keywordstyle=\color{Peach},       % keyword style
  morekeywords={kable,*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  %numberstyle=\tiny\color{gray}, % the style that is used for the line-numbers
  rulecolor=\color{white},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
      % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  mathescape=true,
  escapechar=|
  }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%        


        
\makeatletter
\let\thetitle\@title
\let\theauthor\@author
\let\thedate\@date
\makeatother

%%% commandes mise en page %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%        
\newcommand{\ld}{\log_{2}}
\newcommand{\R}{\mathbbm{R}}
\newcommand{\N}{\mathbbm{N}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\E}{\mathbbm{E}}
\newcommand{\V}{\mathbbm{V}}
\newcommand{\prob}{\mathbbm{P}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\Xti}{\widetilde{X_i}}
\newcommand{\Xtj}{\widetilde{X_j}}
\newcommand{\Xn}{\overline{X_n}}
\newcommand{\gn}{\hat{g_n}}
\newcommand{\n}{\mathcal{N}}
\newcommand{\lv}{\mathcal{L}}
\newcommand{\thetat}{\tilde{\theta}}
\newcommand{\indep}{\perp \!\!\! \perp}

\newcommand{\console}[1]{\colorbox{black}{\begin{minipage}[c]{1\linewidth}\textcolor{white}{\texttt{#1}}\end{minipage}}}

\newtheorem{prop}{Proposition}
\newtheorem{thm}{Théorème}
\newtheorem{cor}{Corollaire}
\newtheorem{lem}{Lemme}
\newtheorem{hyp}{Hypothèse}
\theoremstyle{definition}\newtheorem{defn}{Définition}
\theoremstyle{definition}\newtheorem{exm}{Exemple}
\theoremstyle{definition}\newtheorem{nota}{Notation}
\theoremstyle{definition}\newtheorem{rem}{Remarque}

\renewcommand{\qedsymbol}{\adfhangingflatleafright}


\begin{document}
%%% Pour l'annexe
\def\appendixpage{\vspace*{8cm}
\begin{center}
\Huge\textbf{Annexes}
\end{center}
}
\def\appendixname{Annexe}%
%%%

%%% Page de garde %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
\begin{center}
\includegraphics[scale=0.6]{logo.png}
\hfill
\includegraphics[scale=0.35]{fds_logo.png}\\[3cm]
\linespread{1.2}\huge {\bfseries Projet Master 2 SSD }\\[0.5cm]
\linespread{1.2}\LARGE {\bfseries Crowd-Sourcing}\\[1.5cm]
\linespread{1}

{\large Rédigé par\\}
{\Large \textsc{pralon} Nicolas}\\
{\Large \textsc{thiriet} Aurelien}\\
{\large \emph{Encadrant :} Joseph \textsc{Salmon}}\\[1.5cm] 

\includegraphics[scale=0.7]{imag_logo.png}

\end{center}
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}
\addcontentsline{toc}{part}{Introduction}

En science appliquée et notamment en statistique inférentielle, le recueil de données constitue une étape primordiale, il nous permet d'élaborer des modèles et de construire des estimateurs. Il convient à ce titre d'être attentif à l'observation des données.

Dans un cadre idéal, les modèles construit nécessitent un faible nombre de données dont l'observation peut être réalisée par des professionnels du domaine concerné. Toujours est-il que ces situations sont peu courantes et l'observation, par ces experts, d'un grand nombre de données n'est pas envisageable. C'est à ce titre que le "Crowd-Sourcing" est couramment utilisé, puisqu'un grand nombre d'intervenant permet de construire la base de donnée nécessaire. Toute fois des erreurs d'obervations quant à la nature des données peut-être commisent, il est alors essentiel de s'accorder sur une décision.

C'est à ce problème, que ce projet propose de présenter différentes solutions. Nous étudierons le modèle construit par DAWID et SKENE.


\newpage

\chapter{Modélisation du problème}

\section{Modèle Dawid-Skene}

Dans ce chapitre, nous établissons le cadre nous permettant de traiter le problème de décision lorsque plusieurs obervations d'une même donnée sont fournit. 

Nous définissons alors l'exemple suivant : 

\begin{itemize}[label=\adfflowerleft]
	\item On dispose d'un esemble de patient $\left\{1,\cdots,I\right\}$ tous atteint d'une maladie, et on suppose le vecteur aléatoire $(X_i)_{i \in \{1,\cdots,I \}}$, tel que $\forall i$, $X_i$ à valeur dans $\left\{1,\cdots,J\right\}$ représente la maladie du partient $i$.
	\item On dispose egalement d'un esemble de médecin $\{1, \cdots, K\}$, et on considère $\forall i \in \{1, \cdots, I\}$ le vecteur aléatoire $(Y^k_i)_{k \in \{1, \cdots, K\}}$ le diagnostique du médecin $k$ au patient $i$, $Y^k_i$ à valeur dans $\{0,1\}^J$. 
	\item Chaque médecin répond indépendament des autres et la maladie de chaque patient est indépendante des autres. On note l'independance 2 à 2, $(Y^k_i)_k \indep$ et $(X_i)_i \indep$
\end{itemize}

Afin de diagnostiquer le meilleur traitement à chaque patient, on doit leur diagnostiquer une maladie. Toute fois nous pouvons nous retrouver dans la situation où le diagnostique différent d'un médecin à un autre, pour diverses raisons; erreur de mesure, erreur d'annotation, fatigue. Il convient dans ce cas de trouver une solution à la question : quelle maladie est atteint chaque patient ? \\

Dans le but de simplifier l'approche, nous considérons dans un premier temps le cas d'un medecin et d'un patient, nous pouvons également émettre l'hypothèse vraisemblable suivante : \\

\begin{hyp}
	Nous supposons que, $\forall j,k \in \llbracket 1,J \rrbracket \times \llbracket 1,K \rrbracket$, $Y^k_i | X_i = \alpha_i \sim Multinomiale((\pi^k_{\alpha_i,j})_{j \in \llbracket 1,J \rrbracket},1)$
\end{hyp}

Sachant que le partient $i$ est atteint de la maladie $\alpha_i$, la réponse du médecin $k$ suit une loi multinomiale. Le médecin peut, ou non, commettre une érreur de mesure même en connaissant la vraie maladie du patient. \\

Les paramètres $(\pi^k_{\alpha_i,j})_{j \in \llbracket 1,J \rrbracket}$ correspondent à la probabilité que le médecin $k$ diagnostique la maladie $j$ au patient $i$ sachant qu'il est atteint de la vraie maladie $\alpha_i$. On suppose que chaque médecin ne voit qu'une seule fois chaque patient.\\

On ainsi
\begin{center}
$\prob_{Y^k_i|X_i = \alpha_i} = \displaystyle \sum_{(n^k_{i,1}, \cdots, n^k_{i,J})} \underbrace{\frac{\big(\displaystyle \sum_{j=1}^J n^k_{i,j}\big)!}{\displaystyle \prod_{j=1}^J (n^k_{i,j})!}\displaystyle \prod_{j=1}^J \big(\pi^k_{\alpha_i,j}\big)^{n^k_{i,j}}}_{\textit{densité suivant} \displaystyle \sum \delta_{(n^k_{i,1}, \cdots, n^k_{i,J})}} \delta_{(n^k_{i,1}, \cdots, n^k_{i,J})}$
\end{center}

Avec $n^k_{i,j}$ le nombre de fois que le médecin $k$ à diagnostiqué la maladie $j$ au patient $i$\\

On a alors que la vraisemblance de $Y^k_i | X_i = \alpha_i$ est équivalent à $\displaystyle \prod_{j=1}^J \big(\pi^k_{\alpha_i,j}\big)^{n^k_{i,j}}$\\


\begin{rem}
	Ici on a une dépendance des paramètres $(\pi^k_{\alpha_i,j})$ en $k$,$j$ et $\alpha_i$ (pour le conditionnement), mais l'indice $i$ sert simplement à indiquer qu'on considère le patient $i$ et ne consitue pas un paramètre.\\
	Cela signifie que, sachant que le patient $i$ est malade de la maladie $\alpha_i$ et que le patient $i'$ est malade de la maladie $\alpha_{i'}$ mais $\alpha_i = \alpha_{i'}$, la probabilité que le médecin $k$ diagnostique $j$ au patient $i$ est la même de celle où il diagnostque $j$ pour le patient $i'$. 
	En tout état de connaissance, le médecin sachant que les deux patients sont atteint de la même maladie, la probabilité qu'il diagnostique la maladie $j$ à l'un, et $j$ à l'autre n'a pas lieu d'être différente. Il s'agit pour le médecin d'une même situation et le choix ne dépend que de lui. 
\end{rem}
\bigskip


Dans le but de se ramener au cas de plusieurs patients et plusieurs médecins, on considère ici un patient $i$ et tous les médecins $(1, \cdots, K)$.\\

\begin{rem}
	Par soucis de lisibilité on notera $\prob(Y^k_i = (n^k_{i,1}, \cdots, n^k_{i,J})|X_i = \alpha_i)$ par $\prob(Y^k_i|X_i = \alpha_i).$
\end{rem}

Pour tous les médecins on a dans ce cas : 
\begin{center}
	\begin{align*}
		\prob \left( \displaystyle \bigcap_{k=1}^K Y^k_i |X_i =\alpha_i \right) &= \displaystyle \prod_{k=1}^K \prob \left(Y^k_i |X_i = \alpha_i \right) \textit{, car} (Y^k_i)_k \indep\\
	&\propto \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{\alpha_i,j}\right)^{n^k_{i,j}}\\
	\end{align*}
\end{center}

La vraisemblance du vecteur $(Y^k_i | X_i = \alpha_i)_k$ est équivalente à cette même valeur.\\

En finalité, les résultats obervés sont ceux du vecteur $(Y^k_i)_k$ pour un patient i fixé. Toute fois, si nous avions également accès à la vraie maladie du patient, on observe dans ce cas les résultats du vecteur $((Y^k_i)_k, X_i)$.\\

Intéréssons nous alors, à la probabilité $\prob \left( \left( \displaystyle \bigcap_k Y^k_i \right) \bigcap \left(X_i = \alpha_i \right) \right)$.\\

On note $\forall i,j \in \{1, \cdots, J\}^2, p_j = \prob(X_i = j)$.\\

\begin{center}
	\begin{align*}
		\prob \left( \left( \displaystyle \bigcap_k Y^k_i \right) \bigcap \left(X_i = \alpha_i \right) \right) &= \prob \left( \displaystyle \bigcap_{k = 1}^K Y^k_i |X_i = \alpha_i \right) \times \prob(X_i = \alpha_i)\\
		& \propto p_{\alpha_i} \times \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{\alpha_i,j}\right)^{n^k_{i,j}}\\
	\end{align*}
\end{center}

Plus généralement, en définissant $T_{i,l}$ la variable aléatoire telle que $T_{i,l} = 1$, si la maladie du patient $i$ est $l$ et $T_{i,l} = 0$ sinon, on a : \\

\begin{center}
	$p_{\alpha_i} \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{\alpha_i,j}\right)^{n^k_{i,j}} = \displaystyle \prod_{l=1}^J \left( p_l \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{l,j}\right)^{n^k_{i,j}} \right)^{T_{i,l}}$
\end{center}

De plus l'événement $\{X_i = \alpha_i \} = \{T_{i,\alpha_i} = 1\}$, $\forall l \in \{1, \cdots, J\}$ $T_{i,l}$ est alors connu.

\newpage

Il manque maintenant à se placer dans le cas où on observe les résutats des médecins pour touts les patients, et les maladies des patients.\\

$\forall k, (Y^k_i) \indep X_{i'}, i' \neq i$ et $(Y^k_i)_i \indep $, on a alors $Y^k_i \indep (Y^k_{i'},X_{i'})$ puis $(Y^k_i,X_i) \indep (Y^k_{i'},X_{i'})$\\
On a ainsi la probabilité : 

\begin{center}
	\begin{align*}
		\prob \left( \left( \displaystyle \bigcap_i \displaystyle \bigcap_k Y^k_i \right) \cap \left( \displaystyle \bigcap_i Xi = \alpha_i \right) \right) &= \prob \left( \displaystyle \bigcap_i \left[ \left( \displaystyle \bigcap_k Y^k_i \right) \cap \left(X_i = \alpha_i\right) \right] \right)\\
	&= \displaystyle \prod_i \prob \left( \left( \displaystyle \bigcap_k Y^k_i \right) \cap \left( X_i = \alpha_i \right) \right)\\
	&\propto \displaystyle \prod_i^I \displaystyle \prod_{l=1}^J \left( p_l \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{l,j}\right)^{n^k_{i,j}} \right)^{T_{i,l}}\\
	\end{align*}
\end{center}

La vraisemblance est également équivalente à cette même expréssion.

\section{Résolution du problème}

Pour faire un choix quant à la maladie de chaque patient, la démarche que l'on adopte est la suivante :\\

Soit le patient $i$, on décide que ce patient est atteint de la maladie $j$ si : \\
\begin{center}
	$j = argmax_j (\prob(X_i=j)=p_j=\prob(T_{i,j}=1))$
\end{center}


En pratique, on ne connaît aucun des $(p_j)_j$, mais on peut les estimer par maximum de vraisemblance du modèle défini dans la partie précédente.\\
Un problème tout fois est toujours présent, la vraisemblance du modèle nécessite de connaître les valeurs des $T_{i,l}$ qui ne sont à priori pas connu, puisqu'au quel cas on connaîtrait la maladie du patient.\\
Dans un premier temps considérons connu les valeurs de $T{i,l}$ et déterminons les estimateurs du maximum de vraisemblance, on se concentrera dans un second temps à comment déterminer les $T_{i,l}$.
\bigskip

En effectuant la maximisation du Lagrangien de la fonction : 

\begin{center}
	$\left( \left(\pi^k_{l,j}\right)_{l,j,k},(p_j)_j \right) |  \!\!\! \longrightarrow \displaystyle \prod_i^I \displaystyle \prod_{l=1}^J \left( p_l \displaystyle \prod_{k=1}^K \displaystyle \prod_{j = 1}^J \left(\pi^k_{l,j}\right)^{n^k_{i,j}} \right)^{T_{i,l}}$
\end{center}

Sous la contrainte $\forall l,k \in  \llbracket 1,J \rrbracket \times  \llbracket 1,K \rrbracket, \left(\displaystyle \sum_{j = 1}^J \pi^k_{l,j} = 1 \right)$\\

On obtient les estimateurs du maximum de vraisemblance suivant : \\

\begin{center} 
	$\widehat{\pi^{k}_{l,j}} = \displaystyle \frac{\displaystyle \sum_i T_{i,l} n^k_{i,j}}{\displaystyle \sum_l \displaystyle \sum_i T_{i,l} n^k_{i,j}}$
\end{center}
\begin{center}
	$\widehat{p_l} = \displaystyle \frac{\displaystyle \sum_i T_{i,l}}{I}$
\end{center}

\begin{rem}
	La preuve alourdissant la lecture, on se contente de donner l'idée générale permettant de retrouver les estimateurs. \\
Il convient de faire ressortir de la fonction ci-dessus, pour l'estimation des $\pi^k_{l,j}$ (respectivement des $p_j$), le terme en $\pi^k_{l,j}$ (repectivement en $p_l$). On obtient alors les termes suivant $\displaystyle \prod_i^I \left(\pi^k_{l,j}\right)^{n^k_{i,j}T_{i,l}} a_i$ et $\displaystyle \prod_i^I (p_l)^{T_{i,l}} b_i$ avec $a_i$ et $b_i$ correspondant au reste du produit.\\
Les contraintes de maximisation permettent ensuite d'obtenir les dénominateurs dans l'expression des estimateurs. 
\end{rem}
\bigskip
\bigskip
\bigskip

A présent que l'on dispose des estimateurs du maximum de vraisemblance, il nous faut déterminer les $T_{i,l}$, nécessaire au calcul des estimateurs. Ces derniers n'étant pas connu, on souhaite les estimer. \\
Un estimateur naturel pouvant être prit pour chaque $T_{i,l}$ est l'espérance conditionnelle sachant $(Y^k_i)_k$, $\E[T_{i,l}|(Y^k_i)_k]$. \\
Il s'agit de la meilleure approximation de $T_{i,l}$ au sens de la norme $\mathcal{L}^2$, à partir de ce que l'on connait $<(Y^k_i)_k>$ (les réponses des médecins).\\

$\E[T_{i,l}|(Y^k_i)_k]$ peut se réécrire sous la forme suivante : 

\begin{center}
	\begin{align*}
		\E[T_{i,l}|(Y^k_i)_k] &= \E[0 \times \1_{\{T_{i,l} = 0\}} + 1 \times \1_{\{T_{i,l} = 1\}}|(Y^k_i)_k] \\
		&= \E[\1_{\{T_{i,l} = 1\}}|(Y^k_i)_k]\\
		&= \prob\left( T_{i,l} = 1 |(Y^k_i)_k \right)\\
	\end{align*}
\end{center}

En explicitant cette probabilité on a : 

\begin{center}
	\begin{align*}
		\prob\left( T_{i,l} = 1 |(Y^k_i)_k \right) &= \displaystyle \frac{\prob \left( \left( T_{i,l} = 1 \right) \cap \left(Y^k_i\right)_k\right)}{\prob \left( \left( Y^k_i \right)_k \right)}\\
		&= \displaystyle \frac{\prob \left( \left(Y^k_i \right)_k | \left( T_{i,l} =1 \right) \right) \times \prob (T_{i,l} = 1)}{\displaystyle \sum_l^J \prob \left( \left(Y^k_i \right)_k | \left( X_i = l \right) \right) \times \prob (X_i = l)}\\
		&= \displaystyle \frac{\prob \left( \left(Y^k_i \right)_k | \left( X_i = l \right) \right) \times \prob (X_i = l)}{\displaystyle \sum_l^J \prob \left( \left(Y^k_i \right)_k | \left( X_i = l \right) \right) \times \prob (X_i = l)}\\
		&= \displaystyle \frac{p_l \displaystyle \prod_k^K \displaystyle \prod_j^J (\pi^k_{l,j})^{n^k_{i,j}}}{\displaystyle \sum_l^J p_l \displaystyle \prod_k^K \displaystyle \prod_j^J (\pi^k_{l,j})^{n^k_{i,j}}}, \textit{car } \forall i,l ~\{X_i = l\} = \{T_{i,l} = 1\}\\
	\end{align*}
\end{center}

On a donc réussi à obtenir une estimation des $T_{i,l}$, mais celle-ci dépend des paramètres recherchés. Ceci ne nous permet pas, dans ce cas, de donner les estimateurs du maximum de vraisemblance des $(\pi^k_{l,j})_{k,l,j}$ et $p_l$. \\
Toute fois, on voit alors apparaître une procédure permettant d'estimer les estimateurs du maximum de vraisemblance, il s'agit dans un premier temps, d'estimer arbitrairement les $T_{i,l}$ afin d'obtenir une estimation des estimateurs du maximum de vraisemblance $(\pi^k_{l,j})_{k,l,j}$ et $p_l$.\\
On peut alors donner une estimation des $\E[T_{i,l}|(Y^k_i)_k]$, et de nouveau obtenir une estimation des estimateurs du maximum de vraisemblance des $(\pi^k_{l,j})_{k,l,j}$ et $p_l$. Cette procédure itérative coorespond à ce que l'on appelle l'algoritme EM.\\

Il n'existe pas de preuvre de convergence de la suite de paramètres établie par l'algorithme EM et le chois de bons paramètres initiaux est de fait primordial.\\
Toute fois, une des garanties théoriques est fondamentale à l'utilisation de l'algoritme EM ; Les estimateurs ainsi obtenus ont la particularités de faire croitre, à chaque itération, la vraisemblance des observations. \\
Comme sus-mentionné, ce résultat ne prétend pas que l'algorithme EM maximisera la vraisemblance des observations. Ce dernier peut tout à fait produire une suite de paramètres correspondant à un maximum local ; et une augmentation du nombre d'itérations n'apportera pas de solution à ce problème.

\end{document}
